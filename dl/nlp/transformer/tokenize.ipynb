{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入预训练分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faizalfeng/Code/algorithm_study/.venv/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_load import train_data_raw, test_data_raw, valid_data_raw\n",
    "import pandas as pd\n",
    "\n",
    "train_df, test_df, vad_df = pd.DataFrame(train_data_raw), pd.DataFrame(test_data_raw), pd.DataFrame(valid_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取训练数据中的src和trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_src_trg(df: pd.DataFrame):\n",
    "    df = df.assign(\n",
    "        de=df['translation'].apply(lambda x: x['de']), en=df['translation'].apply(lambda x: x['en'])).drop(columns=['translation']\n",
    "    ).head(100000)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src_trg, test_src_trg, vad_src_trg = build_src_trg(train_df), build_src_trg(test_df), build_src_trg(vad_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 微调分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = train_src_trg['de'].tolist() + train_src_trg['en'].tolist()\n",
    "test_corpus = test_src_trg['de'].tolist() + test_src_trg['en'].tolist()\n",
    "valid_corpus = vad_src_trg['de'].tolist() + vad_src_trg['en'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建新词汇表\n",
    "from collections import Counter\n",
    "\n",
    "new_tokens = []\n",
    "for text in train_corpus + test_corpus + valid_corpus:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    new_tokens.extend(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = [token for token, freq in counter.items() if freq > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(new_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizer/tokenizer_config.json',\n",
       " './tokenizer/special_tokens_map.json',\n",
       " './tokenizer/vocab.json',\n",
       " './tokenizer/source.spm',\n",
       " './tokenizer/target.spm',\n",
       " './tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将语句分割成idx并进行填充和截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_pad(row: pd.Series):\n",
    "    return tokenizer(row, padding=\"max_length\", truncation=True, return_tensors='pt', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df: pd.DataFrame):\n",
    "    return df.assign(src_input=lambda x: x['de'].apply(tokenizer_pad).apply(lambda x: x['input_ids']), trg_input=lambda x: x['en'].apply(tokenizer_pad).apply(lambda x: x['input_ids']), src_mask=lambda x: x['de'].apply(tokenizer_pad).apply(lambda x: x['attention_mask']), trg_mask=lambda x: x['en'].apply(tokenizer_pad).apply(lambda x: x['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, valid_data = get_data(train_src_trg), get_data(test_src_trg), get_data(vad_src_trg)\n",
    "# train_feature, train_mask = train_data['src_input'], train_data['src_mask']\n",
    "# train_label = train_data['trg_input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[tensor(15418), tensor(886), tensor(492), ten...\n",
       "1       [[tensor(3207), tensor(6437), tensor(920), ten...\n",
       "2       [[tensor(605), tensor(7554), tensor(1678), ten...\n",
       "3       [[tensor(8961), tensor(39369), tensor(11201), ...\n",
       "4       [[tensor(71), tensor(692), tensor(14070), tens...\n",
       "                              ...                        \n",
       "2998    [[tensor(198), tensor(2413), tensor(12070), te...\n",
       "2999    [[tensor(188), tensor(982), tensor(5045), tens...\n",
       "3000    [[tensor(609), tensor(1522), tensor(11007), te...\n",
       "3001    [[tensor(364), tensor(3361), tensor(2683), ten...\n",
       "3002    [[tensor(6160), tensor(45), tensor(3490), tens...\n",
       "Name: src_input, Length: 3003, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_src, train_mask = train_data['src_input'], train_data['src_mask']\n",
    "test_src = test_data['src_input']\n",
    "test_src"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
