{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pretrained tokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokenizer info to json\n",
    "tokenizer.save('./tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 100, 1010, 1061, 1005, 2035, 999, 100, 2024, 2017, 100, 1029, 102],\n",
       " [101, 7592, 1010, 1061, 1005, 2035, 999, 100, 2024, 2017, 100, 1029, 102])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretrained-tokenizer encode\n",
    "output_1 = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
    "output_2 = tokenizer.encode(\"hello, y'all! How are you üòÅ ?\")\n",
    "output_1.ids, output_2.ids\n",
    "\n",
    "# you will find that this pretrained-tokenizer will distinguish upper case and lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\", y ' all! are you?\", \"hello, y ' all! are you?\")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretrained-tokenizer encode\n",
    "tokenizer.decode(output_1.ids), tokenizer.decode(output_2.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tokenizer normalizer\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello how are u?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try normalizer on string\n",
    "normalizer.normalize_str(\"H√©ll√≤ h√¥w are √º?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fuck', (0, 4)),\n",
       " ('you', (5, 8)),\n",
       " (',', (8, 9)),\n",
       " ('my', (10, 12)),\n",
       " ('little', (13, 19)),\n",
       " ('pussy', (20, 25))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "pre_tokenizer = Whitespace()\n",
    "pre_tokenizer.pre_tokenize_str(\"Fuck you, my little pussy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
